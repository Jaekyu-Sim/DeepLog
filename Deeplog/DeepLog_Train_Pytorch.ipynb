{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770534e1-3530-4c59-adb9-debccb0dbded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Input data summary ======\n",
      "Loading ./data/HDFS/HDFS_100k.log_structured.csv\n",
      "250 63\n",
      "Slicing 6351 sessions, with window 10\n",
      "Slicing done, 27805 windows generated\n",
      "Slicing 1589 sessions, with window 10\n",
      "Slicing done, 6408 windows generated\n",
      "Train: 27805 windows (1053/27805 anomaly), 26752/27805 normal\n",
      "Test: 6408 windows (253/6408 anomaly), 6155/6408 normal\n",
      "test :  14\n",
      "Epoch 1/15, training loss: 1.01550\n",
      "Epoch 2/15, training loss: 0.33975\n",
      "Epoch 3/15, training loss: 0.30030\n",
      "Epoch 4/15, training loss: 0.28703\n",
      "Epoch 5/15, training loss: 0.28021\n",
      "Epoch 6/15, training loss: 0.27424\n",
      "Epoch 7/15, training loss: 0.27087\n",
      "Epoch 8/15, training loss: 0.26643\n",
      "Epoch 9/15, training loss: 0.26392\n",
      "Epoch 10/15, training loss: 0.26136\n",
      "Epoch 11/15, training loss: 0.26065\n",
      "Epoch 12/15, training loss: 0.25702\n",
      "Epoch 13/15, training loss: 0.25555\n",
      "Epoch 14/15, training loss: 0.25383\n",
      "Epoch 15/15, training loss: 0.25263\n",
      "Train validation:\n",
      "[('window_acc', 0.87833), ('session_acc', 0.96174), ('f1', 0.11636), ('recall', 0.064), ('precision', 0.64)]\n",
      "Test validation:\n",
      "[('window_acc', 0.92962), ('session_acc', 0.95972), ('f1', 0.2), ('recall', 0.12698), ('precision', 0.47059)]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import dataloader\n",
    "from models import DeepLog\n",
    "from preprocessing import Vectorizer, Iterator\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "hidden_size = 32\n",
    "num_directions = 2\n",
    "topk = 5\n",
    "train_ratio = 0.8\n",
    "window_size = 10\n",
    "epoches = 15\n",
    "num_workers = 2\n",
    "device = 0 \n",
    "\n",
    "struct_log = './data/HDFS/HDFS_100k.log_structured.csv' # The structured log file\n",
    "label_file = './data/HDFS/anomaly_label.csv' # The anomaly label file\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    (x_train, window_y_train, y_train), (x_test, window_y_test, y_test), test_buffer = dataloader.load_HDFS(struct_log, label_file=label_file, window='session', window_size=window_size, train_ratio=train_ratio, split_type='uniform')\n",
    "    \n",
    "    feature_extractor = Vectorizer()\n",
    "    train_dataset = feature_extractor.fit_transform(x_train, window_y_train, y_train)\n",
    "    test_dataset = feature_extractor.transform(x_test, window_y_test, y_test)\n",
    "\n",
    "    train_loader = Iterator(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers).iter\n",
    "    test_loader = Iterator(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers).iter\n",
    "    print(\"test : \", feature_extractor.num_labels)\n",
    "    model = DeepLog.DeepLog(num_labels=feature_extractor.num_labels, hidden_size=hidden_size, num_directions=num_directions, topk=topk, device=device)\n",
    "    model.fit(train_loader, epoches)\n",
    "\n",
    "    print('Train validation:')\n",
    "    metrics = model.evaluate(train_loader)\n",
    "\n",
    "    print('Test validation:')\n",
    "    metrics = model.evaluate(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b35111-39da-49ad-8a6f-32e5f07bfbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38223be-d676-4702-bf03-68bd5f703d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b0379-f757-4f30-9b77-9bbd9d3a3bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47905b8e-6302-45a0-8361-32ac9a0a35d6",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2643e8-3167-4f0f-b6c6-edd1a05b233b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f831e8b1-a325-49fc-a0e8-f0ca8144489a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c9b7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b99d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f7d53-e827-4c17-a0c2-249e24b2e273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a31f02e-46a6-4200-adf3-3027df0249e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Input data summary ======\n",
      "Loading ./data/HDFS/HDFS_100k.log_structured.csv\n",
      "250 63\n",
      "Slicing 6351 sessions, with window 10\n",
      "Slicing done, 27805 windows generated\n",
      "Slicing 1589 sessions, with window 10\n",
      "Slicing done, 6408 windows generated\n",
      "Train: 27805 windows (1053/27805 anomaly), 26752/27805 normal\n",
      "Test: 6408 windows (253/6408 anomaly), 6155/6408 normal\n",
      "test :  14\n",
      "Epoch 1/5, training loss: 1.01353\n",
      "Epoch 2/5, training loss: 0.38671\n",
      "Epoch 3/5, training loss: 0.31505\n",
      "Epoch 4/5, training loss: 0.29871\n",
      "Epoch 5/5, training loss: 0.28954\n",
      "Train validation:\n",
      "[('window_acc', 0.8783), ('session_acc', 0.9619), ('f1', 0.14184), ('recall', 0.08), ('precision', 0.625)]\n",
      "Test validation:\n",
      "[('window_acc', 0.93243), ('session_acc', 0.96224), ('f1', 0.25), ('recall', 0.15873), ('precision', 0.58824)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333275e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030462a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7db545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c444576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5be9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbff80c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac982630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da088c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef8d92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333dd2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546808b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08df780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e530433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaekyu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\jaekyu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\jaekyu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\jaekyu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\jaekyu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\jaekyu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell end\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import dataloader\n",
    "from preprocessing import Vectorizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def make_batch(train_data, batch_size = 16):\n",
    "    train_dataset = train_data\n",
    "    num_of_data = len(train_dataset[\"x\"])\n",
    "    \n",
    "    index = np.arange(0, num_of_data)\n",
    "    np.random.shuffle(index)\n",
    "    index = index[:batch_size]\n",
    "    \n",
    "    shuffled_input_data = [train_dataset[\"x\"][i] for i in index]\n",
    "    shuffled_label_data = [train_dataset[\"window_y\"][i] for i in index]\n",
    "    \n",
    "    return np.asarray(shuffled_input_data), np.asarray(shuffled_label_data)\n",
    "\n",
    "def load_data():\n",
    "    struct_log = './data/HDFS/HDFS_100k.log_structured.csv' # The structured log file\n",
    "    label_file = './data/HDFS/anomaly_label.csv' # The anomaly label file\n",
    "    \n",
    "    (x_train, window_y_train, y_train), (x_test, window_y_test, y_test), buffer = dataloader.load_HDFS(struct_log, label_file=label_file, window='session', window_size=window_size, train_ratio=train_ratio, split_type='uniform')\n",
    "    \n",
    "    feature_extractor = Vectorizer()\n",
    "    train_dataset = feature_extractor.fit_transform(x_train, window_y_train, y_train)\n",
    "    test_dataset = feature_extractor.transform(x_test, window_y_test, y_test)\n",
    "    num_label = feature_extractor.num_labels\n",
    "    \n",
    "    return train_dataset, test_dataset, num_label\n",
    "\n",
    "\n",
    "class DeepLog():\n",
    "    def __init__(self, sess, seq_length, batch_size, epochs, lr, hidden_size = 100, topk = 9):\n",
    "        self.sess = sess\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = lr\n",
    "        self.epochs = epochs\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_directions = 2\n",
    "        self.topk = topk\n",
    "        self.num_label = 12\n",
    "        \n",
    "        #self.rnn = nn.LSTM(input_size=1, hidden_size=self.hidden_size, batch_first=True, bidirectional=(self.num_directions==2))\n",
    "        \n",
    "        self.X = tf.placeholder(shape = [None, self.seq_length, 1], dtype = tf.float32)\n",
    "        self.Y = tf.placeholder(shape = [None, 1], dtype = tf.float32)\n",
    "        \n",
    "        self.model()\n",
    "        self.optimizer()\n",
    "        \n",
    "        print(\"init end\")\n",
    "    \n",
    "    def model(self):\n",
    "        cell_fw = tf.contrib.rnn.LSTMBlockCell(num_units=self.hidden_size)\n",
    "        cell_bw = tf.contrib.rnn.LSTMBlockCell(num_units=self.hidden_size)\n",
    "        \n",
    "        \n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, self.X, time_major=False, dtype=tf.float32)\n",
    "        #outputs[:,-1,:]\n",
    "        print(\"output's shape : \", np.shape(outputs))\n",
    "        outputs_fw = tf.transpose(outputs[0], [0,1,2])\n",
    "        outputs_bw = tf.transpose(outputs[1], [0,1,2])\n",
    "        print(\"outputs_fw's shape : \", np.shape(outputs_fw))\n",
    "        print(\"outputs_bw's shape : \", np.shape(outputs_bw))\n",
    "        self.logits = tf.layers.Dense(outputs_bw, (self.num_label+1))\n",
    "        #self.y_pred = tf.argmax(self.logits, axis=1)\n",
    "        \n",
    "        return self.logits\n",
    "        \n",
    "        \n",
    "        \n",
    "    def optimizer(self):\n",
    "        self.loss = tf.losses.softmax_cross_entropy(self.Y, self.logits)\n",
    "        self.optimizer_loss = tf.train.AdamOptimizer(learning_rate=lr).minimize(self.total_loss)\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        data_size = 64#28883#41749\n",
    "        batch_size = self.batch_size\n",
    "        total_batch = data_size//batch_size\n",
    "\n",
    "        loss_data = []\n",
    "        \n",
    "        #write = tf.summary.FileWriter('./mygraph', self.sess.graph)\n",
    "        SAVE_PATH = \"C:/Users/jaekyu/Documents/Jupyter Lab/Deeplog/Weight/Weight.ckpt\"\n",
    "        print(\"session start\")\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        print(\"training data load start\")\n",
    "        train_dataset, test_dataset, num_label = load_data()\n",
    "        print(\"training data load finish\")\n",
    "        try:\n",
    "            saver.restore(self.sess, SAVE_PATH)\n",
    "            print(\"load\")\n",
    "        except:\n",
    "            print(\"first training\")\n",
    "            \n",
    "        train_dataset, test_dataset, num_label = load_data()\n",
    "            \n",
    "        for epoch in range(self.epochs):#\n",
    "            print(\"epoch\",epoch+1, \"start\")\n",
    "            for i in range(total_batch):#total_batch\n",
    "                #data load, batch 생성\n",
    "                input_loader, label_loader = make_batch(train_dataset, batch_size = self.batch_size)\n",
    "\n",
    "                #total_loss_opt, Heat_loss, Vector_loss = self.sess.run([self.optimizer_total_loss, self.loss_stage6_branch2, self.loss_stage6_branch1],\n",
    "                #         feed_dict = {self.X : batch_img, self.confidence_map_label : heatmap, self.vector_map_label : vectormap})\n",
    "\n",
    "                #h_loss_data.append(Heat_loss)\n",
    "                #v_loss_data.append(Vector_loss)\n",
    "            #heatmap_output, vectormap_output = self.sess.run([self.stage6_branch2, self.stage6_branch1], feed_dict = {self.X : batch_img})\n",
    "            \n",
    "            \n",
    "            \n",
    "            saver.save(self.sess, SAVE_PATH)\n",
    "            \n",
    "print(\"cell end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f189de-ec56-4473-93db-2d805bf30577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jaekyu\\AppData\\Local\\Temp/ipykernel_18852/1589278835.py:63: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\jaekyu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\jaekyu\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "output's shape :  (2,)\n",
      "outputs_fw's shape :  (?, 10, 100)\n",
      "outputs_bw's shape :  (?, 10, 100)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18852/975795985.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeepLog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m#batch_Img_resized, predicted_heatmap, label_heatmap, label_Anno_Data, categories = obj.test()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18852/1589278835.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sess, seq_length, batch_size, epochs, lr, hidden_size, topk)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18852/1589278835.py\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"outputs_fw's shape : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs_fw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"outputs_bw's shape : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs_bw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs_bw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_label\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;31m#self.y_pred = tf.argmax(self.logits, axis=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m                                 \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                                 \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                                 **kwargs)\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    919\u001b[0m     super(Dense, self).__init__(\n\u001b[0;32m    920\u001b[0m         activity_regularizer=regularizers.get(activity_regularizer), **kwargs)\n\u001b[1;32m--> 921\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Tensor'"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "hidden_size = 32\n",
    "num_directions = 2\n",
    "topk = 5\n",
    "train_ratio = 0.2\n",
    "window_size = 10\n",
    "epoches = 2\n",
    "num_workers = 2\n",
    "device = 0 \n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    obj = DeepLog(sess=sess, seq_length = 10, batch_size = 32, epochs = 3, lr = 0.001, hidden_size = 100, topk = 9)\n",
    "    obj.train()\n",
    "    #batch_Img_resized, predicted_heatmap, label_heatmap, label_Anno_Data, categories = obj.test()\n",
    "    #batch_Img_resized, batch_Anno_Data, batch_Heatmap, gaussian_Heatmap, categories = obj.data_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ac523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
